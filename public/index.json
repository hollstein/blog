[{"content":"Promting or prompt engineering is becoming the new programming paradigm for many NLP tasks. Among other things, it means specifying the prompt for a given task as well as generation parameters such as temperature, top-k or penalty-alpha. This is, in fact, an optimization task over the space of possible prompts, suitable LLMs as well as generation. Some o parameters here are discrete (e.g. top-k) and some are continuous (e.g. temperature). There are ways to directly learn prompts or to fine-tune an LLM on a given task, but this might be more costly and time-consuming compared to simply selecting a set of good enough prompt, LLM and generation parameters (80-20 rule).\nBack to this optimization task: For many instances, there will not be a good metric that one can automatically compute and that is very well aligned with the task at hand. For these cases, manual judgement or labeling will be needed. However, rating outputs consistently on a scale is very difficult and unpractical for many reasons. What should be easy for most humans is to express preference over one outcome versus another one.\nSo, we need an optimization approach that doesn\u0026rsquo;t act on direct function values but on comparisons given by a human, which is efficient in exploring the parameter space to make best use of human time.\nAfter some research on the current state of the art, I\u0026rsquo;m settling with botorch which implements methods for this approach. They way to use it is to imports the used libraries:\nfrom botorch.acquisition.preference import AnalyticExpectedUtilityOfBestOption from botorch.models.pairwise_gp import ( PairwiseGP, PairwiseLaplaceMarginalLogLikelihood ) from botorch.models.transforms.input import Normalize from botorch.optim import optimize_acqf from botorch.fit import fit_gpytorch_mll import numpy as np import torch from itertools import combinations from scipy.stats import multivariate_normal import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) and to give the to be optimized utility function. Here I\u0026rsquo;m using a toy-example which is a multinomial Gaussian, but in a real application a human would express preferences over generated text:\n# data generating helper functions, not to be called directly :-) def __utility(X): \u0026#34;\u0026#34;\u0026#34;Given X, output corresponding utility (i.e., the latent function)\u0026#34;\u0026#34;\u0026#34; global mn return mn.pdf(X) def compare(xx,ii,jj): \u0026#34;\u0026#34;\u0026#34;Costly eval function here\u0026#34;\u0026#34;\u0026#34; return [ x[0] for x in sorted( [(ii,__utility(xx[ii])),(jj,__utility(xx[jj]))], key=lambda x:x[1],reverse=True ) ] Specify some settings:\nNUM_BATCHES = 15 dim = 2 NUM_RESTARTS = 3 RAW_SAMPLES = 512 q = 2 # number of points per query q_comp = 1 # number of comparisons per query noise = 0.01 And run the optimization loop:\nbounds = torch.stack([torch.zeros(dim), torch.ones(dim)]) # user either one #x_max_unknown = np.random.uniform(low=0,high=1,size=dim) # random center point x_max_unknown = np.array([0.492, 0.53]) # put in center so graph looks nice mn = multivariate_normal(mean=x_max_unknown,cov=.05) print(\u0026#34;x max:\u0026#34;,x_max_unknown) print() xx = np.random.uniform(low=0,high=1,size=(q,dim)) # initial random dataset comparisons = np.array( [ compare(xx,ii,jj) for ii,jj in list(combinations(range(len(xx)), 2)) ] ) model = PairwiseGP( torch.tensor(xx), torch.tensor(comparisons), input_transform=Normalize(d=xx.shape[-1]), ) x_max_trace=[] with warnings.catch_warnings(): for j in range(1, NUM_BATCHES + 1): next_X, acq_val = optimize_acqf( acq_function=AnalyticExpectedUtilityOfBestOption(pref_model=model), bounds=bounds, q=q, num_restarts=NUM_RESTARTS, raw_samples=RAW_SAMPLES, ) xx = np.concatenate([xx,next_X.numpy()]) comparisons = np.concatenate([ comparisons, np.array([compare(xx,len(xx)-2,len(xx)-1)])] ) model = PairwiseGP( torch.tensor(xx), torch.tensor(comparisons), input_transform=Normalize(d=xx.shape[-1]), ) i_x_max = int(model.utility.argmax()) x_max = model.datapoints[i_x_max].tolist() x_max_trace.append(x_max) if j%2==0: print( f\u0026#34;j={j:02}: {\u0026#39;,\u0026#39;.join([f\u0026#39;{j:.3}\u0026#39; for j in x_max])} \u0026#34; f\u0026#34;-\u0026gt; {float(model.utility[i_x_max]):.4} \u0026#34; f\u0026#34;L2: {((x_max - x_max_unknown)**2).sum():.4f}\u0026#34; ) This is the expected output:\nx max: [0.492 0.53 ] j=02: 0.385,0.323 -\u0026gt; 0.5722 L2: 0.0543 j=04: 0.359,0.518 -\u0026gt; 0.7756 L2: 0.0179 j=06: 0.538,0.466 -\u0026gt; 0.8494 L2: 0.0063 j=08: 0.514,0.486 -\u0026gt; 0.9426 L2: 0.0025 j=10: 0.514,0.486 -\u0026gt; 1.032 L2: 0.0025 j=12: 0.514,0.486 -\u0026gt; 1.048 L2: 0.0025 j=14: 0.514,0.486 -\u0026gt; 1.101 L2: 0.0025 Visualize what is going on:\nimport plotly.express as px import pandas as pd import plotly.graph_objects as go df = pd.DataFrame( [ { \u0026#34;x\u0026#34;:pp[0], \u0026#34;y\u0026#34;:pp[1], \u0026#34;u\u0026#34;:__utility(pp) } for pp in np.random.uniform(low=0,high=1,size=(1500,dim)) ] ) fig = px.scatter(df, x=\u0026#34;x\u0026#34;, y=\u0026#34;y\u0026#34;,color=\u0026#34;u\u0026#34;) fig.update_layout(xaxis=dict(domain=[0, 1.0]), yaxis=dict(domain=[0.0, 1.0])) x,y = x_max ax,ay = x_max fig.add_annotation( xref=\u0026#34;x\u0026#34;, yref=\u0026#34;y\u0026#34;, x=x, y=y, font={\u0026#34;size\u0026#34;:20}, text=f\u0026#34;X\u0026#34;, axref=\u0026#34;x\u0026#34;, ayref=\u0026#34;y\u0026#34;, ax=ax, ay=ay, arrowhead=2, ) fig.add_trace( go.Scatter(x=[x[0] for x in x_max_trace],y=[x[1] for x in x_max_trace],mode=\u0026#34;lines\u0026#34;,name=\u0026#34;opt path\u0026#34;,line={\u0026#34;width\u0026#34;:5}) ) for ii,(ci,cj) in enumerate(comparisons.tolist()): x,y = xx[ci].tolist() ax,ay = xx[cj].tolist() fig.add_annotation( xref=\u0026#34;x\u0026#34;, yref=\u0026#34;y\u0026#34;, x=x, y=y, arrowcolor=\u0026#34;#BCBBB5\u0026#34;, text=f\u0026#34;{ii}\u0026#34;, axref=\u0026#34;x\u0026#34;, ayref=\u0026#34;y\u0026#34;, ax=ax, ay=ay, arrowhead=2, ) fig.show() We see that the algorithm picks comparison points to update the expected optimum point in parameter space in a quite efficient way.\n","permalink":"http://localhost:1313/posts/black_box_optimization_from_comparisons/","summary":"\u003cp\u003ePromting or prompt engineering is becoming the new programming paradigm for many NLP tasks. Among other things, it means specifying the prompt for a given task as well as generation parameters such as temperature, top-k or penalty-alpha. This is, in fact, an optimization task over the space of possible prompts, suitable LLMs as well as generation. Some o parameters here are discrete (e.g. top-k) and some are continuous (e.g. temperature). There are ways to directly learn prompts or to fine-tune an LLM on a given task, but this might be more costly and time-consuming compared to simply selecting a set of good enough prompt, LLM and generation parameters (80-20 rule).\u003c/p\u003e","title":"Optimizing black box functions purely from A / B comparisons"},{"content":"Developing AI / ML application behind company IT infrastructure can be challenging. When developing prototypes or quickly checking the latest models from the Huggingface hub, I\u0026rsquo;m often struck by SSL errors. Often, the only way out for the moment it to disable SSL locally for specific actions, also: Don\u0026rsquo;t do this outside of quick experiments or in production systems.\nAn excellent resource for dealing with these errors is this Stack Overflow thread: https://stackoverflow.com/questions/15445981/how-do-i-disable-the-security-certificate-check-in-python-requests\nJust for reference, I\u0026rsquo;m reproducing Blenders answer here and show how to use it. What makes this one so good, is that it implements a solution as a context manager which isolates turning off SSL to specific code blocks:\nimport warnings import contextlib import requests from urllib3.exceptions import InsecureRequestWarning old_merge_environment_settings = requests.Session.merge_environment_settings @contextlib.contextmanager def no_ssl_verification(): opened_adapters = set() def merge_environment_settings(self, url, proxies, stream, verify, cert): # Verification happens only once per connection so we need to close # all the opened adapters once we\u0026#39;re done. Otherwise, the effects of # verify=False persist beyond the end of this context manager. opened_adapters.add(self.get_adapter(url)) settings = old_merge_environment_settings(self, url, proxies, stream, verify, cert) settings[\u0026#39;verify\u0026#39;] = False return settings requests.Session.merge_environment_settings = merge_environment_settings try: with warnings.catch_warnings(): warnings.simplefilter(\u0026#39;ignore\u0026#39;, InsecureRequestWarning) yield finally: requests.Session.merge_environment_settings = old_merge_environment_settings for adapter in opened_adapters: try: adapter.close() except: pass An example of using transformers:\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM with no_ssl_verification(): tokenizer = AutoTokenizer.from_pretrained(\u0026#34;[MODEL NAME]\u0026#34;) model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#34;[MODEL NAME]\u0026#34;) ","permalink":"http://localhost:1313/posts/ssl_errors_and_company_proxies/","summary":"\u003cp\u003eDeveloping AI / ML application behind company IT infrastructure can be challenging. When developing prototypes or quickly checking the latest models from the Huggingface hub, I\u0026rsquo;m often struck by SSL errors. Often, the only way out for the moment it to disable SSL locally for specific actions, also: Don\u0026rsquo;t do this outside of quick experiments or in production systems.\u003c/p\u003e\n\u003cp\u003eAn excellent resource for dealing with these errors  is this Stack Overflow thread: \u003ca href=\"https://stackoverflow.com/questions/15445981/how-do-i-disable-the-security-certificate-check-in-python-requests\"\u003ehttps://stackoverflow.com/questions/15445981/how-do-i-disable-the-security-certificate-check-in-python-requests\u003c/a\u003e\u003c/p\u003e","title":"SSL errors and company proxies"},{"content":"Recently I added these comments to my prototype text extraction code from documents:\nimport pytesseract # this works really bad pytesseract.pytesseract.tesseract_cmd = \u0026#34;/usr/bin/tesseract\u0026#34; import easyocr # looking much better reader = easyocr.Reader([\u0026#39;en\u0026#39;],gpu=False) The types of images I\u0026rsquo;m after are those found in typical Power Point slides in a business context. So mostly digital fonts and almost nothing handwritten. I gave both tesseract and easyocr a try and for my use-case, easyocr is the clear winner.\nIt is as simple as that:\nocr_method = ... # set to your choice prs = Presentation(\u0026#39;file.pptx\u0026#39;) def cleanup_text(text:str) -\u0026gt; str: return re.sub(\u0026#39;\\s+\u0026#39;, \u0026#39; \u0026#39;,re.sub(r\u0026#34;[^\\x20-\\x7E]\u0026#34;, \u0026#34; \u0026#34;, text)).strip() for i_slide,slide in enumerate(prs.slides): for i_shape,shape in enumerate(slide.shapes,start=i_shape): if shape.shape_type == MSO_SHAPE_TYPE.PICTURE: if ocr_method == \u0026#34;tesseract\u0026#34;: try: image_text = cleanup_text( pytesseract.image_to_string( Image.open( io.BytesIO( shape.image.blob ) ) ) ) except TypeError: image_text=\u0026#34;\u0026#34; elif ocr_method == \u0026#34;easyocr\u0026#34;: if shape.image.ext in [\u0026#34;jpg\u0026#34;, \u0026#34;jpeg\u0026#34;, \u0026#34;png\u0026#34;]: try: image_text = \u0026#34; \u0026#34;.join( [ cleanup_text(tt[1]) for tt in reader.readtext(shape.image.blob) if tt[2]\u0026gt;0.8 ] ) except RuntimeError: image_text = \u0026#34;\u0026#34; print(f\u0026#34;OCR Error: {shape.image.ext}\u0026#34;) else: image_text = \u0026#34;\u0026#34; else: raise ValueError() ","permalink":"http://localhost:1313/posts/which_ocr_to_use/","summary":"\u003cp\u003eRecently I added these comments to my prototype text extraction code from documents:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e pytesseract  \u003cspan style=\"color:#75715e\"\u003e# this works really bad\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epytesseract\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003epytesseract\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etesseract_cmd \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/usr/bin/tesseract\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e easyocr  \u003cspan style=\"color:#75715e\"\u003e# looking much better\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ereader \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e easyocr\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eReader([\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;en\u0026#39;\u003c/span\u003e],gpu\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe types of images I\u0026rsquo;m after are those found in typical Power Point slides in a business context. So mostly digital fonts and almost nothing handwritten. I gave both \u003ca href=\"https://pypi.org/project/pytesseract/\"\u003etesseract\u003c/a\u003e and \u003ca href=\"https://pypi.org/project/easyocr/\"\u003eeasyocr\u003c/a\u003e a try and for my use-case, easyocr is the clear winner.\u003c/p\u003e\n\u003cp\u003eIt is as simple as that:\u003c/p\u003e","title":"Which OCR to use today? (extracting text from images in power point)"},{"content":"Enabling the features of current age open source LLMs on MS Office documents can be an efficiency game changer in many business tasks. The first step is to extract text from these documents before feeding the LLMs. The initial answer on how to do it from ChatGPT and You Chat was almost working, but there was missing text from grouped shapes. This code snipped does extract and does a little bit of cleanup:\nimport re from pptx import Presentation from pptx.enum.shapes import MSO_SHAPE_TYPE def is_good_text(text): # adapt to what makes sense if len(text) \u0026lt; 3: return False else: return True def cleanup_text(text:str) -\u0026gt; str: # collapse duplicate spaces, remove non printable character return re.sub(\u0026#39;\\s+\u0026#39;, \u0026#39; \u0026#39;,re.sub(r\u0026#34;[^\\x20-\\x7E]\u0026#34;, \u0026#34; \u0026#34;, text)).strip() prs = Presentation(\u0026#34;file.pptx\u0026#34;) notes_texts, slide_texts = [], [] for i_slide,slide in enumerate(prs.slides): # process slide notes notes_texts.append( \u0026#34;\\n\u0026#34;.join([shape.text for shape in slide.notes_slide.shapes if is_good_text(shape.text)]) if slide.has_notes_slide else \u0026#34;\u0026#34; ) slide_texts += [ cleanup_text(text) for text in ( [ # Text from ungrouped shapes shape.text for shape in slide.shapes if hasattr(shape, \u0026#39;text\u0026#39;) and is_good_text(shape.text) ] + [# Text from grouped shapes shape.text for group_shape in slide.shapes if group_shape.shape_type == MSO_SHAPE_TYPE.GROUP for shape in group_shape.shapes if shape.has_text_frame and is_good_text(shape.text) ] ) ] ","permalink":"http://localhost:1313/posts/extraxt_all_text_from_pptx_files/","summary":"\u003cp\u003eEnabling the features of current age open source LLMs on MS Office documents can be an efficiency game changer in many business tasks. The first step is to extract text from these documents before feeding the LLMs. The initial answer on how to do it from ChatGPT and You Chat was almost working, but there was missing text from grouped shapes. This code snipped does extract and does a little bit of cleanup:\u003c/p\u003e","title":"Extraxt all text from pptx files with python"},{"content":"Developing date science use cases on a Mac with 256GB disk space is fun compared to doing this on Windows, but the small disk of my Mac can make things challenging. In case the disk is full again:\ndocker system prune --all, removes all thing\u0026rsquo;s docker, also everything cached, so initial build times are a price to pay If I have no clue why this disk is full: find . -maxdepth 1 -type d -mindepth 1 -exec du -hs {} \\; pip cache purge conda clean --all Removing unused software also helps ","permalink":"http://localhost:1313/posts/managing_disk_space_on_mac/","summary":"\u003cp\u003eDeveloping date science use cases on a Mac with 256GB disk space is fun compared to doing this on Windows, but the small disk of my Mac can make things challenging. In case the disk is full again:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003edocker system prune --all\u003c/code\u003e, removes all thing\u0026rsquo;s docker, also everything cached, so initial build times are a price to pay\u003c/li\u003e\n\u003cli\u003eIf I have no clue why this disk is full: \u003ccode\u003efind . -maxdepth 1 -type d -mindepth 1 -exec du -hs {} \\;\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip cache purge\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda clean --all\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eRemoving unused software also helps\u003c/li\u003e\n\u003c/ul\u003e","title":"Managing disk space on Mac"},{"content":"I\u0026rsquo;m building some docker images to deploy a data science use-case to AWS as ECS task. One trick to reduce startup-time and external dependencies is to include all needed models into the image. This increases the image size but decreases the time spent getting the models from an external dependency and is safeguarding the service for external resources becoming unavailable. In this project, I can\u0026rsquo;t rely on some other internal way of fetching the models.\nThis is how I do it for SentenceTransformer:\nRUN python -c \u0026#34;import os; os.environ[\u0026#39;CURL_CA_BUNDLE\u0026#39;] = \u0026#39;\u0026#39;; from sentence_transformers import SentenceTransformer; SentenceTransformer(\u0026#39;sentence-transformers/paraphrase-mpnet-base-v2)\u0026#34; spaCy models\nRUN python -m spacy download en_core_web_sm and Hugginface models:\nRUN python -c \u0026#34;import os; os.environ[\u0026#39;CURL_CA_BUNDLE\u0026#39;] = \u0026#39;\u0026#39;; from transformers import pipeline; model = pipeline(task=\u0026#39;text2text-generation\u0026#39;, model=\u0026#39;declare-lab/flan-alpaca-gpt4-xl\u0026#39;); print(model)\u0026#34; I was fighting SSL errors, and the only way of getting this done was to downgrade requests\u0026lt;2.27.1 (details here) and to include import os; os.environ['CURL_CA_BUNDLE'] = ''. This is certainly a bad idea in production and should be used with extreme case.\n","permalink":"http://localhost:1313/posts/ssl_issues_docker/","summary":"\u003cp\u003eI\u0026rsquo;m building some docker images to deploy a data science use-case to AWS as ECS task. One trick to reduce startup-time and external dependencies is to include all needed models into the image. This increases the image size but decreases the time spent getting the models from an external dependency and is safeguarding the service for external resources becoming unavailable. In this project, I can\u0026rsquo;t rely on some other internal way of fetching the models.\u003c/p\u003e","title":"SSL issues when baking LLMs into docker images"},{"content":"Prompting instruction fine-tuned LLMs allows getting NLP tasks done deemed infeasible some years ago and prompt hacking (trial and error until good enough on some examples) and prompt engineering (same as hacking but with tracking and metrics) become an interesting programming paradigm.\nWhat works for me Models A good publicly available model is an instruction fine-tuned flan-t5 model from the hub. This space evolves fast, I consider this outdated after Q2/2023.\nCreating prompts Delimiters are the clean code version from prompting. I found that they will not be the deal maker or breaker, but the code creating the prompt looks so much nicer and cleaner this way.\nprompt = f\u0026#34;\u0026#34;\u0026#34; [Detailed and specific task description] the text delimited by triple backticks. ```{text}``` \u0026#34;\u0026#34;\u0026#34; Resources Video lectures by deeplearing.ai: Good intro, videos better watched on 2x, nice demo notebooks dair-ai open book on github Awesome-Prompt-Engineering GitHub list ","permalink":"http://localhost:1313/posts/on_prompting/","summary":"\u003cp\u003ePrompting instruction fine-tuned LLMs allows getting NLP tasks done deemed infeasible some years ago and prompt hacking (trial and error until good enough on some examples) and prompt engineering (same as hacking but with tracking and metrics) become an interesting programming paradigm.\u003c/p\u003e\n\u003ch2 id=\"what-works-for-me\"\u003eWhat works for me\u003c/h2\u003e\n\u003ch3 id=\"models\"\u003eModels\u003c/h3\u003e\n\u003cp\u003eA good publicly available model is an instruction fine-tuned flan-t5 \u003ca href=\"https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl\"\u003emodel\u003c/a\u003e from the hub. This space evolves fast, I consider this outdated after Q2/2023.\u003c/p\u003e","title":"On prompting"},{"content":"Debugging Dash apps can be fun and painful at the same time.\nFor debugging, I often litter the code with these to have the more powerful IPython available for debugging.\nfrom IPython import embed; embed() Beware of the auto-update in debug mode. If we run the app in debug mode\napp.run(host=\u0026#39;0.0.0.0\u0026#39;, port=8091, debug=True) we get hot reloading which is super useful. However, if this happens while beeing in a debug window, accessing and killing the process becomes tricky. What works for me is this (adjust port accordingly):\nkill $(lsof -t -i :8091) ","permalink":"http://localhost:1313/posts/debugging_dash_apps/","summary":"\u003cp\u003eDebugging \u003ca href=\"https://dash.plotly.com/\"\u003eDash apps\u003c/a\u003e can be fun and painful at the same time.\u003c/p\u003e\n\u003cp\u003eFor debugging, I often litter the code with these to have the more powerful IPython available for debugging.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e IPython \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e embed; embed()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBeware of the auto-update in debug mode. If we run the app in debug mode\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eapp\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erun(host\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0.0.0.0\u0026#39;\u003c/span\u003e, port\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e8091\u003c/span\u003e, debug\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ewe get hot reloading which is super useful. However, if this happens while beeing in a debug window, accessing and killing the process becomes tricky. What works for me is this (adjust port accordingly):\u003c/p\u003e","title":"Debugging dash apps"},{"content":"\u0026ldquo;Why We Sleep\u0026rdquo; by Matthew Walker Ph.D. is a great read to bring you up to speed about what we scientifically know about sleep today. He describes the results and implications of many years of research, and doesn\u0026rsquo;t shy away from describing how the research experiments were done and designed. Fascinating read.\nQuick messages to take away:\nA good amount of healthy sleep is so important for overall health and lifespan that it is rational to prioritize getting it. Negatively impacting healthy sleep is: Drinking alcohol late, if not drinking alcohol is not an option, this is a healthy argument for moderate day drinking. Same goes for caffeine, don\u0026rsquo;t consume coffee late. Even if you think is not impacting your sleep (amount), the sleep itself is not functioning as good as it should be. Sleep research experiments and research is fascinating. ","permalink":"http://localhost:1313/posts/book_why_we_sleep/","summary":"\u003cp\u003e\u0026ldquo;Why We Sleep\u0026rdquo; by \u003ca href=\"https://www.sleepdiplomat.com/author\"\u003eMatthew Walker Ph.D.\u003c/a\u003e is a great read to bring you up to speed about what we scientifically know about sleep today. He describes the results and implications of many years of research, and doesn\u0026rsquo;t shy away from describing how the research experiments were done and designed. Fascinating read.\u003c/p\u003e\n\u003cp\u003eQuick messages to take away:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA good amount of healthy sleep is so important for overall health and lifespan that it is rational to prioritize getting it.\u003c/li\u003e\n\u003cli\u003eNegatively impacting healthy sleep is:\n\u003cul\u003e\n\u003cli\u003eDrinking alcohol late, if not drinking alcohol is not an option, this is a healthy argument for moderate day drinking.\u003c/li\u003e\n\u003cli\u003eSame goes for caffeine, don\u0026rsquo;t consume coffee late. Even if you think is not impacting your sleep (amount), the sleep itself is not functioning as good as it should be.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSleep research experiments and research is fascinating.\u003c/li\u003e\n\u003c/ul\u003e","title":"Just finished reading: 'Why we sleep'"},{"content":"Today, I used the great library python-docx to create a rather complex report in MS Word. Why Word? It was the quickest way to get my results into the hands of colleagues from the business. I was using a not so large LLM locally on my GPU server to generate a report for decision-making around some or our relevant topics. In the mix went the remarkable transfomrers library as well as BERTopic for clustering. The library is easy to use for getting things done fast. Documentation is a bit lacking, and ChatGPT gets many questions about python-docx wrong. What is possible, but hard, to do is creating links within the document or creating a table of contents.\n","permalink":"http://localhost:1313/posts/python_docx_is_great/","summary":"\u003cp\u003eToday, I used the great library \u003ca href=\"https://python-docx.readthedocs.io/en/latest/user/install.html\"\u003epython-docx\u003c/a\u003e to create a rather complex report in MS Word. Why Word? It was the quickest way to get my results into the hands of colleagues from the business. I was using a not so large LLM locally on my GPU server to generate a report for decision-making around some or our relevant topics. In the mix went the remarkable \u003ca href=\"https://pypi.org/project/transformers/\"\u003etransfomrers\u003c/a\u003e library as well as \u003ca href=\"https://maartengr.github.io/BERTopic/index.html\"\u003eBERTopic\u003c/a\u003e for clustering. The library is easy to use for getting things done fast. Documentation is a bit lacking, and ChatGPT gets many questions about python-docx wrong. What is possible, but hard, to do is creating links within the document or creating a table of contents.\u003c/p\u003e","title":"python-docx is a great library for writing MS Word files"},{"content":"Using LLMs safely keeps me up at night, especially when working for a pharma company where our decisions can impact patients. This area still needs tremendous amounts of research, but seeing things like NeMo Guardrails looks promising. The guardrails are programmed in the language of LLMs, plain English, which in principle allows evolving these guardrails fast. How to manage these guardrails to not become an entangled mess is left to be seen.\n","permalink":"http://localhost:1313/posts/nvidia_nemo_guardrails/","summary":"\u003cp\u003eUsing LLMs safely keeps me up at night, especially when working for a pharma company where our decisions can impact patients. This area still needs tremendous amounts of research, but seeing things like \u003ca href=\"https://github.com/NVIDIA/NeMo-Guardrails\"\u003eNeMo Guardrails\u003c/a\u003e looks promising. The guardrails are \u003cem\u003eprogrammed\u003c/em\u003e in the language of LLMs, plain English, which in principle allows evolving these guardrails fast. How to manage these guardrails to not become an entangled mess is left to be seen.\u003c/p\u003e","title":"Nvidia publishes NeMo guardrails to use LLMs more safely"},{"content":"Listening to this episode of the wonderful Python Bytes really shocked me with covering this blog post of the python software foundation: https://pyfound.blogspot.com/2023/04/the-eus-proposed-cra-law-may-have.html\nI totally missed it and hope it is not too late that members of the European Parliament can resolve these issues with this law. If this becomes law, the future of free and open software in Europe looks bleak, and we don\u0026rsquo;t want that. This is my letter to members of the European Parliament, taking on responsibility for Berlin (in German):\nBetreff: Konsequenzen des CRA Gesetzes für Python und Open Source\nSehr geehrter Herr / Frau \u0026hellip;,\nich schreibe Ihnen heute in Ihrer Funktion als Abgeordneter im Europaparlament und meiner Rolle als Bürger der Stadt Berlin und langjähriger Nutzer und Entwickler von Open-Source-Software. Ich arbeite aktuell als Lead Data Scientist für eine große Pharma-Firma und bin aktiv an der Digitalisierung und Verbesserung unserer Geschäftsprozesse beteiligt. Dabei setze ich Methoden des maschinellen Lernens und der künstlichen Intelligenz ein. Open-Source-Software macht dies erst möglich und Entwicklungen, die hier förderlich sind, sind es auch für meine Arbeit.\nMit großem Erschrecken habe ich folgenden Blogpost der renommierten Python Software Foundation über das geplante Gesetz CRA gelesen: https://pyfound.blogspot.com/2023/04/the-eus-proposed-cra-law-may-have.html\nIch teile die Einschätzung der Python Software Foundation, dass der aktuelle Vorschlag schwerwiegende negative Auswirkungen auf das Python-Softwareökosystem und Open-Source-Software allgemein haben wird. Verteiler von Freier Software, die diese unentgeltlich zur Verfügung stellen, sollten nicht von den neuen Regeln erfasst werden. Das Gleiche sollte für Entwickler gelten, die dies unentgeltlich, individuell oder als Hobby in ihrer Freizeit tun. Bitte setzen Sie sich dafür ein, dass das CRA in seiner aktuellen Form nicht verabschiedet wird. Setzen Sie sich dafür ein, dass Hobbyentwickler und gemeinnützige Organisationen wie die Python Software Foundation vom CRA ausgenommen werden und weiterhin die Ergebnisse ihrer Arbeit der Allgemeinheit zur Verfügung stellen können. Wenn das CRA so umgesetzt wird, könnte es dazu kommen, dass niemand mehr in Europa das persönliche Risiko tragen kann, Open-Source-Software zu entwickeln und zu veröffentlichen. Der gigantische Nutzen dieser Software wäre damit in Europa nicht mehr verfügbar und dies würde für uns einen erheblichen Wettbewerbsnachteil bedeuten. Ihr Einsatz unterstützt die Kultur der Open-Source-Software, damit wir die Digitalisierung in Deutschland und der Welt gemeinsam vorantreiben und die großen Vorteile und Chancen der Digitalisierung weiterhin mit der Allgemeinheit teilen können. Voller Hoffnung und mit besten Grüßen,\n\u0026hellip;\n","permalink":"http://localhost:1313/posts/cra_law_berlin_mep/","summary":"\u003cp\u003eListening to \u003ca href=\"https://pythonbytes.fm/episodes/show/332/a-python-a-slurpee-and-some-chaos\"\u003ethis\u003c/a\u003e episode of the wonderful Python Bytes really shocked me with covering this blog post of the python software foundation: \u003ca href=\"https://pyfound.blogspot.com/2023/04/the-eus-proposed-cra-law-may-have.html\"\u003ehttps://pyfound.blogspot.com/2023/04/the-eus-proposed-cra-law-may-have.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eI totally missed it and hope it is not too late that members of the European Parliament can resolve these issues with this law. If this becomes law, the future of free and open software in Europe looks bleak, and we don\u0026rsquo;t want that. This is my letter to members of the European Parliament, taking on responsibility for Berlin (in German):\u003c/p\u003e","title":"My letter to Berlin members of the european parliament regarding the CRA law proposal"},{"content":"I haven\u0026rsquo;t seen a field (NLP) change this rapid in such a short period of time during my professional career. This is fascinating times indeed. These are my current predictions and mental models:\nLLMs hare here to stay: They are useful, useful things will find their nice and stick around. Their prevalence and if one notices them as such is much harder to predict. It will take some time until Simon Willisons mental model of calculator for words will become common sense. When this happens, the hype will turn into a more pragmatic search for useful applications. (This is more of a hope than a prediction.) LLMs will become Commodity: Meaning there is little competitive advantage gained from private closed source models. If private models emerge which provide such benefits, open-source models will emerge with the same or higher level of usefulness and replace the closed ones. Mature: there is a Cambrian explosion going on in the field right now, at some point the emerging ecology of models will have colonized the space and the rate of change will slow down. There will be a sweet spot of computation cost (which related to real costs) and usefulness where persistent models find their nice. Time of maturity: Stabilization and stagnation of the filed will occur by the end of 2024 That being said, the likelihood of each of them being true is small, but this is just the nature of predictions about the real world.\n","permalink":"http://localhost:1313/posts/llm-prediction/","summary":"\u003cp\u003eI haven\u0026rsquo;t seen a field (NLP) change this rapid in such a short period of time during my professional career. This is fascinating times indeed. These are my current predictions and mental models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLLMs hare here to stay: They are useful, useful things will find their nice and stick around. Their prevalence and if one notices them as such is much harder to predict.\u003c/li\u003e\n\u003cli\u003eIt will take some time until Simon Willisons mental model of \u003ca href=\"https://simonwillison.net/2023/Apr/2/calculator-for-words/\"\u003ecalculator for words\u003c/a\u003e will become common sense. When this happens, the hype will turn into a more pragmatic search for useful applications. (This is more of a hope than a prediction.)\u003c/li\u003e\n\u003cli\u003eLLMs will become\n\u003cul\u003e\n\u003cli\u003eCommodity: Meaning there is little competitive advantage gained from private closed source models. If private models emerge which provide such benefits, open-source models will emerge with the same or higher level of usefulness and replace the closed ones.\u003c/li\u003e\n\u003cli\u003eMature: there is a Cambrian explosion going on in the field right now, at some point the emerging ecology of models will have colonized the space and the rate of change will slow down. There will be a sweet spot of computation cost (which related to real costs) and usefulness where persistent models find their nice.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTime of maturity: Stabilization and stagnation of the filed will occur by the end of 2024\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThat being said, the likelihood of each of them being true is small, but this is just the nature of predictions about the real world.\u003c/p\u003e","title":"LLM Predictions"},{"content":"Markdown documents for technical documentation can quickly get out of control and lose their value if one just can\u0026rsquo;t find relevant content. Having a table of content helps to navigate and parse files, even if they become longer and more complex. Markdown itself doesn\u0026rsquo;t create auto-generating and auto-updating table of contents and doing this manual will just not work with real people and real live scenarios where time is always in short supply, especially then documentation is an afterthought to getting features shipped. Automating this is a great way to save time and keep these documents useful.\nOne way that worked for me is the tool markdown-toc, install it via:\nnpm install --save markdown-toc Add \u0026lt;!-- toc --\u0026gt; to a markdown file (e.g. Readme.md) and run:\nnpx markdown-toc -i Readme.md on the file. Done.\n","permalink":"http://localhost:1313/posts/toc-markdown/","summary":"\u003cp\u003eMarkdown documents for technical documentation can quickly get out of control and lose their value if one just can\u0026rsquo;t find relevant content. Having a table of content helps to navigate and parse files, even if they become longer and more complex. Markdown itself doesn\u0026rsquo;t create auto-generating and auto-updating table of contents and doing this manual will just not work with real people and real live scenarios where time is always in short supply, especially then documentation is an afterthought to getting features shipped. Automating this is a great way to save time and keep these documents useful.\u003c/p\u003e","title":"Generating a table of content for mardown files"}]