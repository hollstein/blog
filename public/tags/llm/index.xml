<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Llm on blog</title>
    <link>http://localhost:1313/tags/llm/</link>
    <description>Recent content in Llm on blog</description>
    <generator>Hugo -- 0.141.0</generator>
    <language>en</language>
    <lastBuildDate>Wed, 14 Jun 2023 10:27:55 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Optimizing black box functions purely from A / B comparisons</title>
      <link>http://localhost:1313/posts/black_box_optimization_from_comparisons/</link>
      <pubDate>Wed, 14 Jun 2023 10:27:55 +0200</pubDate>
      <guid>http://localhost:1313/posts/black_box_optimization_from_comparisons/</guid>
      <description>&lt;p&gt;Promting or prompt engineering is becoming the new programming paradigm for many NLP tasks. Among other things, it means specifying the prompt for a given task as well as generation parameters such as temperature, top-k or penalty-alpha. This is, in fact, an optimization task over the space of possible prompts, suitable LLMs as well as generation. Some o parameters here are discrete (e.g. top-k) and some are continuous (e.g. temperature). There are ways to directly learn prompts or to fine-tune an LLM on a given task, but this might be more costly and time-consuming compared to simply selecting a set of good enough prompt, LLM and generation parameters (80-20 rule).&lt;/p&gt;</description>
    </item>
    <item>
      <title>On prompting</title>
      <link>http://localhost:1313/posts/on_prompting/</link>
      <pubDate>Sat, 29 Apr 2023 16:56:41 +0200</pubDate>
      <guid>http://localhost:1313/posts/on_prompting/</guid>
      <description>&lt;p&gt;Prompting instruction fine-tuned LLMs allows getting NLP tasks done deemed infeasible some years ago and prompt hacking (trial and error until good enough on some examples) and prompt engineering (same as hacking but with tracking and metrics) become an interesting programming paradigm.&lt;/p&gt;
&lt;h2 id=&#34;what-works-for-me&#34;&gt;What works for me&lt;/h2&gt;
&lt;h3 id=&#34;models&#34;&gt;Models&lt;/h3&gt;
&lt;p&gt;A good publicly available model is an instruction fine-tuned flan-t5 &lt;a href=&#34;https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl&#34;&gt;model&lt;/a&gt; from the hub. This space evolves fast, I consider this outdated after Q2/2023.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM Predictions</title>
      <link>http://localhost:1313/posts/llm-prediction/</link>
      <pubDate>Sun, 23 Apr 2023 13:29:37 +0200</pubDate>
      <guid>http://localhost:1313/posts/llm-prediction/</guid>
      <description>&lt;p&gt;I haven&amp;rsquo;t seen a field (NLP) change this rapid in such a short period of time during my professional career. This is fascinating times indeed. These are my current predictions and mental models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLMs hare here to stay: They are useful, useful things will find their nice and stick around. Their prevalence and if one notices them as such is much harder to predict.&lt;/li&gt;
&lt;li&gt;It will take some time until Simon Willisons mental model of &lt;a href=&#34;https://simonwillison.net/2023/Apr/2/calculator-for-words/&#34;&gt;calculator for words&lt;/a&gt; will become common sense. When this happens, the hype will turn into a more pragmatic search for useful applications. (This is more of a hope than a prediction.)&lt;/li&gt;
&lt;li&gt;LLMs will become
&lt;ul&gt;
&lt;li&gt;Commodity: Meaning there is little competitive advantage gained from private closed source models. If private models emerge which provide such benefits, open-source models will emerge with the same or higher level of usefulness and replace the closed ones.&lt;/li&gt;
&lt;li&gt;Mature: there is a Cambrian explosion going on in the field right now, at some point the emerging ecology of models will have colonized the space and the rate of change will slow down. There will be a sweet spot of computation cost (which related to real costs) and usefulness where persistent models find their nice.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Time of maturity: Stabilization and stagnation of the filed will occur by the end of 2024&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That being said, the likelihood of each of them being true is small, but this is just the nature of predictions about the real world.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
